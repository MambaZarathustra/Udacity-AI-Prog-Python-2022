## BUILD MODEL ARCHITECTURE ##

# Use GPU if it is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define pre-trained model to be used, ResNet101
# (fc): Linear(in_features=2048, out_features=1000, bias=True)
model = models.resnet101(pretrained=True)

# Freeze model 'features' paramaters
for param in model.parameters():
    param.requires_grad = False

# Redefine the 'classifier' from the pre-trained model
model.classifier = nn.Sequential(nn.Linear(2048, 1000),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(1000, 256),
                                 nn.ReLU(),
                                 nn.Dropout(0.3),
                                 nn.Linear(256, 2),
                                 nn.LogSoftmax(dim=1))

# Define the criterion for loss
criterion = nn.NLLLoss()

# Optimize the weights, but only for the classifier
optimizer = optim.Adam(model.classifier.parameters(), lr=0.002)

# Set the device in which memory of weights to be stored
model.to(device)

####################################################################################################

## TRAIN NETWORK and VALIDATE 

# Set epochs hyperparameter & Initial running loss
epochs = 2
running_loss = 0

# Initialize # of steps for Eval check
steps = 0
print_every = 10


for e in range(epochs):
    
    # Training pass for each epoch
    for images, labels in trainloader:
        
        # Keep track of 'steps' for testing 'if' clause
        steps += 1
        
        # Move images, labels tensors to 'device' memory
        images, labels = images.to(device), labels.to(device)
        
        # Clean accumulated gradients from last pass
        optimizer.zero_grad()
        
        # Forward pass, loss calc, backward pass, update weights
        log_ps = model(images)
        loss = criterion(log_ps, labels)
        loss.backward
        optimizer.step()
        
        running_loss = loss.item()
        
        # Test model every 'print_every' steps (irrespective of epoch)
        # using validation dataset
        if steps % print_every == 0:
            
            # Initialize test loss and accuracy variables
            valid_loss = 0
            accuracy = 0
            
            # Turn off gradients for Eval pass of the model
            with torch.no_grad():
            
                # Trun off gradients for Eval pass of the model
                model.eval()

                # Start Eval pass w/ its own set of images w/in the batch
                for valid_img, valid_labels in validatorloader:

                    # Move to valid_img, valid_labels tensors to 'device' memory
                    valid_img, valid_labels = valid_img.to(device), valid_labels.to(device)

                    # 1 forward pass, loss calc for the batch of images
                    log_ps = model(valid_img)
                    loss = criterion(log_ps, valid_labels)

                    # Add up validation pass losses
                    valid_loss = loss.item()

                    # Calculate accuracy
                    # Convert log(softmax) prob to prob distribution
                    ps = torch.exp(log_ps)

                    # Get 'top_class' from 'topk' and equivocate the prediction
                    # and valid_labels tensor (i.e. same dimensions)
                    # 1 = 1st largest value in prob distribution
                    top_k, top_class = ps.topk(1, dim=1)
                    equals = top_class == valid_labels.view(*top_class.shape)

                    # Convert 'equals' to a FloatTensor, then calc mean
                    accuracy += torch.mean(equals.type(torch.FloatTensor))

            # Print data on this Training and Validation pass
            print(f"Epoch {e+1}/{epochs}.."
                  f"Training loss: {running_loss/print_every:.3f}.. "
                  f"Validation loss: {valid_loss/len(validatorloader):.3f}.."
                  f"Validation accuracy: {accuracy/len(validatorloader):.3f}..")
            
            
            # Reset running loss to 0
            running_loss = 0
                  
            # Set model backto training mode
            model.train()
                
                
