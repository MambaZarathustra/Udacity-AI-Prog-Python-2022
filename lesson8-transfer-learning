## BUILD MODEL ARCHITECTURE ##

# Use GPU if it is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define pre-trained model to be used, ResNet101
# (fc): Linear(in_features=2048, out_features=1000, bias=True)
model = models.resnet101(pretrained=True)

# Freeze model 'features' paramaters
for param in model.parameters():
    param.requires_grad = False

# Redefine the 'classifier' from the pre-trained model
model.classifier = nn.Sequential(nn.Linear(2048, 1000),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(1000, 256),
                                 nn.ReLU(),
                                 nn.Dropout(0.3),
                                 nn.Linear(256, 2),
                                 nn.LogSoftmax(dim=1))

# Define the criterion for loss
criterion = nn.NLLLoss()

# Optimize the weights, but only for the classifier
optimizer = optim.Adam(model.classifier.parameters(), lr=0.002)

# Set the device in which memory of weights to be stored
model.to(device)

####################################################################################################

## TRAIN AND EVAL THE NETWORK ##

# Set epochs hyperparameter
epochs = 5

# Initialize running loss 
running_loss = 0

# Initializ # of steps for Eval check
steps = 0
print_every = 5

for e in range(epochs):
    
    # Training pass for each epoch
    for images, labels in trainloader:
        
        # Keep track of 'steps' for test 'if' clause
        steps += 1
        
        # Move input, labels tensors to the 'device' memory
        images, labels = images.to(device), labels.to(device)
        
        # Clear accumulated gradients from the last pass
        optimizer.zero_grad()
        
        # Forward pass, loss calc, backward pass, update weights 
        log_ps = model.forward(images)
        loss = criterion(log_ps, labels)
        loss.backward
        optimizer.step()
        
        running_loss += loss.item()
        
        # Test the model every 'print_every' steps (irrespective of epoch)
        if steps % print_every == 0:
            
            # Initialize test_loss and accuracy variables
            test_loss = 0
            accuracy = 0
            
            # Turn off gradients for Eval pass of the model
            with torch.no_grad():
                
                # Put model into Eval mode, i.e. no dropout
                model.eval()
                
                # Start Eval pass with its set of images w/in batch
                for test_img, labels in testloader:
                    
                    # Move input, labels tensors to the 'device'
                    test_img, labels = test_img.to(device), labels.to(device)
                    
                    # 1 fwd pass and loss calc for this batch of images
                    log_ps = model.forward(test_img)
                    image_loss = criterion(log_ps, labels)
                    
                    # Add up test loss
                    test_loss += image_loss.item()
                    
                    
                    ## Calculate the accuracy
                    # Convert log (softmax) probabilities to prob distribution
                    ps = torch.exp(log_ps)
                    
                    # Get 'top_class' from 'topk' and equivocate the prediction
                    # and label tensor (i.e. same dimensions)
                    # 1 means 1st largest value in our probabilities
                    top_k, top_class = ps.topk(1, dim=1)
                    equals = top_class == labels.view(*top_class.shape)
                    
                    # Convert 'equals' into a FloatTensor, then calc mean
                    accuracy += torch.mean(equals.type(torch.FloatTensor)) #.item() ??
                    
                
            # Print data on this Training and Testing pass
            # Total loss / # of batches = average loss
            print(f"Epoch {e+1}/{e}.."
                  f"Train loss: {running_loss/print_every:.3f}.."
                  f"Test loss: {test_loss/len(testloader):.3f}.."
                  f"Test Accuracy: {accuracy/len(testloader):.3f}..")

            # Reset running loss to 0 ?????
            running_loss = 0

            # Set model back to train mode
            model.train()
