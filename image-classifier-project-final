###### IMPORT STATEMENTS ######

import torch
from torch import nn, optim
from torchvision import datasets, transforms, models

###### LOAD THE DATA ######

data_dir = 'flowers'
train_dir = data_dir + '/train'
valid_dir = data_dir + '/valid'
test_dir = data_dir + '/test'

###### TODO: Define your transforms for the training, validation, and testing sets ######

train_transforms = transforms.Compose([transforms.RandomRotation(43),
                                      transforms.RandomResizedCrop(224),
                                      transforms.RandomHorizontalFlip(p=0.3),
                                      transforms.RandomGrayscale(),
                                      transforms.RandomVerticalFlip(p=0.1),
                                      transforms.ToTensor(),
                                      transforms.Normalize([0.485, 0.456, 0.406],
                                                           [0.229, 0.224, 0.225])])

valid_transforms = transforms.Compose([transforms.Resize(224),
                                     transforms.CenterCrop(224),
                                     transforms.ToTensor(),
                                     transforms.Normalize([0.485, 0.456, 0.406],
                                                           [0.229, 0.224, 0.225])])

test_transforms = transforms.Compose([transforms.Resize(224),
                                     transforms.CenterCrop(224),
                                     transforms.ToTensor(),
                                     transforms.Normalize([0.485, 0.456, 0.406],
                                                           [0.229, 0.224, 0.225])])

# TODO: Load the datasets with ImageFolder
train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transforms)
test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)


# TODO: Using the image datasets and the trainforms, define the dataloaders
trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=36, shuffle=True)
validatorloader = torch.utils.data.DataLoader(valid_dataset, batch_size=36)
testloader = torch.utils.data.DataLoader(test_dataset, batch_size=36)

###### LABEL MAPPING ######

import json

with open('cat_to_name.json', 'r') as f:
    cat_to_name = json.load(f)

###### # TODO: Build and train your network ######

## Define device for GPU vs CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

## Define pre-trained model
model = models.resnet50(pretrained=True)

## Freeze 'feature' parameters
for param in model.parameters():
    param.requires_grad = False

# Define Model Architecture
# (fc): Linear(in_features=2048, out_features=1000, bias=True)
model.classifier = nn.Sequential(nn.Linear(2048,1024),
                                 nn.ReLU(),
                                 nn.Dropout(0.3),
                                 nn.Linear(1024,512),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(512,102),
                                 nn.LogSoftmax(dim=1))

# Define criterion for the loss
criterion = nn.NLLLoss()

# Define optimizer for backpropagation
optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)

# Move weights to memory of the the active device (GPU/CPU)
model.to(device)

## TRAIN NETWORK and VALIDATE 

# Set epochs hyperparameter & Initial running loss
epochs = 5
running_loss = 0

# Initialize # of steps for EVal check
steps = 0
print_every = 5


for e in range(epochs):
    
    # Training pass for each epoch
    for images, labels in trainloader:
        
        # Keep track of 'steps' for testing 'if' clause
        steps += 1
        
        # Move images, labels tensors to 'device' memory
        images, labels = images.to(device), labels.to(device)
        
        # Clean accumulated gradients from last pass
        optimizer.zero_grad()
        
        # Forward pass, loss calc, backward pass, update weights
        log_ps = model(images)
        loss = criterion(log_ps, labels)
        loss.backward
        optimizer.step()
        
        running_loss = loss.item()
        
        # Test model every 'print_every' steps (irrespective of epoch)
        # using validation dataset
        if steps % print_every == 0:
            
            # Initialize test loss and accuracy variables
            valid_loss = 0
            accuracy = 0
            
            # Trun off gradients for Eval pass of the model
            model.eval()
            
            # Start Eval pass w/ its own set of images w/in the batch
            for valid_img, valid_labels in validatorloader:
                
                # Move to valid_img, valid_labels tensors to 'device' memory
                valid_img, valid_labels = valid_img.to(device), valid_labels.to(device)
                
                # 1 forward pass, loss calc for the batch of images
                log_ps = model(valid_img)
                loss = criterion(log_ps, valid_labels)
                
                # Add up validation pass losses
                valid_loss = loss.item()
                
                # Calculate accuracy
                # Convert log(softmax) prob to prob distribution
                ps = torch.exp(log_ps)
                
                # Get 'top_class' from 'top_k' and equivocate the prediction
                # and valid_labels tensor (i.e. same dimensions)
                # 1 = 1st largest value in prob distribution
                top_k, top_class = ps.topk(1, dim=1)
                equals = top_class == labels.view(*top_class.shape)
                
                # Convert 'equals' to a FloatTensor, then calc mean
                accuracy += torch.mean(equals.type(torch.FloatTensor))
                
            # Print data on this Training and Validation pass
            print(f"Epoch: {e+1}/{e}.."
                  f"Training loss: {running_loss/print_every:.3f}.. "
                  f"Validation loss: {valid_loss/validatorloader:.3f}.."
                  f"Validation accuracy: {accuracy/validatorloader:.3f}..")
            
            # Reset running loss to 0
            running_loss = 0
                  
            # Set model backto training mode
            model.train()
                
                
                


