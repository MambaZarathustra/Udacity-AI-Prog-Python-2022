######################## IMPORT STATEMENTS ########################

import torch
from torch import nn, optim
from torchvision import datasets, transforms, models

######################## LOAD THE DATA ########################

data_dir = 'flowers'
train_dir = data_dir + '/train'
valid_dir = data_dir + '/valid'
test_dir = data_dir + '/test'

###### TODO: Define your transforms for the training, validation, and testing sets ######

train_transforms = transforms.Compose([transforms.RandomRotation(43),
                                      transforms.RandomResizedCrop(224),
                                      transforms.RandomHorizontalFlip(p=0.3),
                                      transforms.RandomGrayscale(),
                                      transforms.RandomVerticalFlip(p=0.1),
                                      transforms.ToTensor(),
                                      transforms.Normalize([0.485, 0.456, 0.406],
                                                           [0.229, 0.224, 0.225])])

valid_transforms = transforms.Compose([transforms.Resize(224),
                                     transforms.CenterCrop(224),
                                     transforms.ToTensor(),
                                     transforms.Normalize([0.485, 0.456, 0.406],
                                                           [0.229, 0.224, 0.225])])

test_transforms = transforms.Compose([transforms.Resize(224),
                                     transforms.CenterCrop(224),
                                     transforms.ToTensor(),
                                     transforms.Normalize([0.485, 0.456, 0.406],
                                                           [0.229, 0.224, 0.225])])

# TODO: Load the datasets with ImageFolder
train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transforms)
test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)


# TODO: Using the image datasets and the trainforms, define the dataloaders
trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=36, shuffle=True)
validatorloader = torch.utils.data.DataLoader(valid_dataset, batch_size=36)
testloader = torch.utils.data.DataLoader(test_dataset, batch_size=36)

######################## LABEL MAPPING ########################

import json

with open('cat_to_name.json', 'r') as f:
    cat_to_name = json.load(f)

######################## # TODO: Build and train your network ########################

## Define device for GPU vs CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

## Define pre-trained model
model = models.resnet50(pretrained=True)

## Freeze 'feature' parameters
for param in model.parameters():
    param.requires_grad = False

# Define Model Architecture
# (fc): Linear(in_features=2048, out_features=1000, bias=True)
model.classifier = nn.Sequential(nn.Linear(2048,1024),
                                 nn.ReLU(),
                                 nn.Dropout(0.3),
                                 nn.Linear(1024,512),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(512,102),
                                 nn.LogSoftmax(dim=1))

# Define criterion for the loss
criterion = nn.NLLLoss()

# Define optimizer for backpropagation
optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)

# Move weights to memory of the the active device (GPU/CPU)
model.to(device)

## TRAIN NETWORK and VALIDATE 

# Set epochs hyperparameter & Initial running loss
epochs = 5
running_loss = 0

# Initialize # of steps for EVal check
steps = 0
print_every = 5


for e in range(epochs):
    
    # Training pass for each epoch
    for images, labels in trainloader:
        
        # Keep track of 'steps' for testing 'if' clause
        steps += 1
        
        # Move images, labels tensors to 'device' memory
        images, labels = images.to(device), labels.to(device)
        
        # Clean accumulated gradients from last pass
        optimizer.zero_grad()
        
        # Forward pass, loss calc, backward pass, update weights
        log_ps = model(images)
        loss = criterion(log_ps, labels)
        loss.backward
        optimizer.step()
        
        running_loss = loss.item()
        
        # Test model every 'print_every' steps (irrespective of epoch)
        # using validation dataset
        if steps % print_every == 0:
            
            # Initialize test loss and accuracy variables
            valid_loss = 0
            accuracy = 0
            
            # Trun off gradients for Eval pass of the model
            model.eval()
            
            # Start Eval pass w/ its own set of images w/in the batch
            for valid_img, valid_labels in validatorloader:
                
                # Move to valid_img, valid_labels tensors to 'device' memory
                valid_img, valid_labels = valid_img.to(device), valid_labels.to(device)
                
                # 1 forward pass, loss calc for the batch of images
                log_ps = model(valid_img)
                loss = criterion(log_ps, valid_labels)
                
                # Add up validation pass losses
                valid_loss = loss.item()
                
                # Calculate accuracy
                # Convert log(softmax) prob to prob distribution
                ps = torch.exp(log_ps)
                
                # Get 'top_class' from 'top_k' and equivocate the prediction
                # and valid_labels tensor (i.e. same dimensions)
                # 1 = 1st largest value in prob distribution
                top_k, top_class = ps.topk(1, dim=1)
                equals = top_class == labels.view(*top_class.shape)
                
                # Convert 'equals' to a FloatTensor, then calc mean
                accuracy += torch.mean(equals.type(torch.FloatTensor))
                
            # Print data on this Training and Validation pass
            print(f"Epoch: {e+1}/{e}.."
                  f"Training loss: {running_loss/print_every:.3f}.. "
                  f"Validation loss: {valid_loss/validatorloader:.3f}.."
                  f"Validation accuracy: {accuracy/validatorloader:.3f}..")
            
            # Reset running loss to 0
            running_loss = 0
                  
            # Set model backto training mode
            model.train()
                
######################## TEST THE NETWORK ########################

# TODO: Do validation on the test set

# Initialize Accuracy
accuracy = 0

# Start Test pass
for test_img, test_labels in testloader:
    
    # Move test_img, test_label tensors to 'device'
    test_img, test_labels = test_img.to(device), test_labels.to(device)
    
    # Forward pass and convert Softmax to prob distribution
    log_ps = model(test_img)
    ps = torch.exp(log_ps)
    
    # Get 'top_class' from 'topk', equivocate
    top_k, top_class = ps.topk(1, dim=1)
    equals = top_class == test_labels.view(*top_class.shape)
    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()
                           


# Print the Accuracy
print(f"Test Accuracy: {accuracy/len(testloader):.3f}")

######################## SAVE CHECKPOINT ########################

# TODO: Save the checkpoint 

model.class_to_idx = train_dataset.class_to_idx

checkpoint = {'input_size': 2048,
              'output_size': 102,
              'network': 'resnet50',
              'classifier': model.classifier,
              'learning_rate': 0.003,
              'optimizer': optimizer.state_dict(),
              'state_dict': model.state_dict(),
              'epochs': epochs,
              'class_to_idx': model.class_to_idx
             }

torch.save(checkpoint, 'checkpoint.pth')

######################## LOAD CHECKPOINT ########################

# TODO: Write a function that loads a checkpoint and rebuilds the model

def load_checkpoint(filepath):
    '''
    Inputs: filepath to the checkpoint (.pth) extension
    Outputs: model.state_dict -- model architecture info, includes parameter matrices for each of the layers
             optimizer -- optimizer parameters
             model.class_to_idx -- 
    '''
    
    # Load the checkpoint
    checkpoint = torch.load(checkpoint)
    
    # Load model archtitecture and parameters
    model = model.load_state_dict(checkpoint['state_dict'])
    optimizer = checkpoint['optimizer']
    model.class_to_idx = checkpoint['class_to_idx']
    # Classifer not retreived from checkpoint, since not needed?

    # Load hyperparameters
    epochs = checkpoint['epochs']
    learning_rate = checkpoint['learning_rate']
    
    # Load other relevent information
    network = checkpoint['network']
    input_size = checkpoint['input_size']
    output_size = checkpoint['output_size']
    
    
    return network, model, optimizer, epochs, learning_rate

######################## ___ ########################




